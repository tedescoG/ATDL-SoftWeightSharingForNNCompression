{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A tutorial on 'Soft weight-sharing for Neural Network compression' \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recently, compression of neural networks has been a much-discussed issue.  One reason for this is the desire to run and store them on mobile devices such as smartphones, robots or [Rasberry Pis](https://github.com/samjabrahams/tensorflow-on-raspberry-pi). Another problem is the energy consumption of a multimillion parameter network. When inference is run at scale the costs can quickly add up.\n",
    "\n",
    "Compressing a given neural network is a challenging task. While the idea of storing weights with low precision has been around for some time, ideas to store the weight matrix in a different storage format are recent. \n",
    "The latter proposal has led to the most successful compression scheme so far. \n",
    "Under the assumption that there is great redundancy within the weights, one can prune most of them. Consequently, only non-zero weights are being stored. These weights can further be compressed by qunatizing them.\n",
    "However, it is not clear how to infer the redundant weights given a trained neural network or to quantize the remaining weights. In the following, we will identify the problem more clearly and review one recent attempt to tackle it. For the following illustrations, I work with MNIST and LeNet-300-100 a simple 2-fully connected neural network with 300 and 100 units.\n",
    "\n",
    "Usually, when training an unregularized neural network, the distribution of weights looks somewhat like a Normal distribution centered at zero. For the proposed storage format this is not ideal. We would like a distribution that is sharply (ideally delta) peaked around some values with significant mass in the zero peak.\n",
    "\n",
    "Weight distribution while common training  | |  Desired distribution    \n",
    ":-------------------------:|:------------:|:-------------------------:\n",
    "![](figures_old/han_pretraining.gif \"title-1\")|<img src=\"./figures/arrow.jpg\" style=\"width: 200px;\"/>| ![](figures_old/han_clustering.png \"title-1\")\n",
    "\n",
    "Following we will shortly review how [Han et. al. (2016)](https://arxiv.org/abs/1510.00149), the proposers of this compression format and current state-of-the-art in compression, tackle the problem. \n",
    "The authors use a multistage pipeline: (i) re-training a pre-trained network with Gaussian prior aka L2-norm on the weights (ii) repeatedly cutting off all weights around a threshold close to zero and after that continue training with L2-norm, and (iii) clustering all weights and retraining again the cluster means.\n",
    "\n",
    "(i)  Re-training with L2 regularization  |  (ii) Repetitiv Cutting and training                           \n",
    "         :-------------------------:|:-------------------------:\n",
    "![](figures_old/han_retraining.gif \"title-1\")|![](figures_old/han_cutandtrain.gif \"title-1\")\n",
    "**(ii) Final stage before clustering**  | **(iii)  Clustering**\n",
    "![](figures_old/han_final_cut.png \"title-1\")|![](figures_old/han_clustering.png \"title-1\")\n",
    "\n",
    "\n",
    "Note that this pipeline is not a differentiable function. Furthermore, pruning and quantization are distinct stages. \n",
    "\n",
    "In contrast, we propose to sparsify and cluster weights in one differentiable retraining procedure. More precisely, we train the network weights with a Gaussian mixture model prior. \n",
    "This is an instance of an empirical Bayesian prior because the parameters in the prior are being learned as well. \n",
    "With this prior present, weights will naturally cluster together since that will allow the gaussian mixture to lower the variance and thus achieve higher probability. \n",
    "It is important, to carefully initialize the learning procedure for those priors because one might end up in a situation where the weights \"chase\" the mixture and the mixture the weights. \n",
    "\n",
    "Note that, even though compression seems to be a core topic of information theory, so far there has been little attention on this angle on things. While in our paper the emphasis lays on this information theoretic view, here we will restrict ourselves to a somewhat practical one.\n",
    "\n",
    "Following, we give a tutorial that shall serve as a practical guide to implementing empirical priors and in particular a Gaussian Mixture with an Inverse-Gamma prior on the variances.  It is divided into 3 parts.\n",
    "\n",
    "\n",
    "* **PART 1:** Pretraining a Neural Network\n",
    "\n",
    "* **PART 2:** Re-train the network with an empirical Gaussian Mixture Prior with Inverse-Gamma hyper-prior on the variances. \n",
    "\n",
    "* **PART 3:** Post-process the re-trained network weights\n",
    "\n",
    "## PART 1: Pretraining a Neural Network \n",
    "\n",
    "\n",
    "First of all, we need a parameter heavy network to compress. In this first part of the tutorial, we train a simple -2 convolutional, 2 fully connected layer- neural network on MNIST with 642K paramters. \n",
    "___________________________\n",
    "\n",
    "We start by loading some essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:48:45.475566Z",
     "start_time": "2025-10-16T09:48:43.931772Z"
    }
   },
   "source": [
    "# IMPORTANT: Disable eager execution for legacy optimizer compatibility\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# from __future__ import print_function  # Not needed in Python 3\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from IPython import display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "______________________\n",
    "Following, we load the MNIST dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:48:45.611372Z",
     "start_time": "2025-10-16T09:48:45.478764Z"
    }
   },
   "source": [
    "from dataset import mnist\n",
    "\n",
    "[X_train, X_test], [Y_train, Y_test], [img_rows, img_cols], nb_classes = mnist()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 60000 train samples and 10000 test samples.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " ___________________________________________________\n",
    "\n",
    "Next, we choose a model. We decide in favor of a  classical 2 convolutional, 2 fully connected layer network with ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:48:45.646997Z",
     "start_time": "2025-10-16T09:48:45.614837Z"
    }
   },
   "source": [
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,  Activation, Flatten, Conv2D\n",
    "\n",
    "# We configure the input here to match the backend. If properly done this is a lot faster. \n",
    "if K.backend() == \"theano\":\n",
    "    InputLayer = Input(shape=(1, img_rows, img_cols), name=\"input\")\n",
    "elif K.backend() == \"tensorflow\":\n",
    "    InputLayer = Input(shape=(img_rows, img_cols,1), name=\"input\")\n",
    "\n",
    "# A classical architecture ...\n",
    "#   ... with 3 convolutional layers,\n",
    "Layers = Conv2D(25, (5, 5), strides=(2,2), activation = \"relu\")(InputLayer)\n",
    "Layers = Conv2D(50, (3, 3), strides=(2,2), activation = \"relu\")(Layers)\n",
    "#   ... and 2 fully connected layers.\n",
    "Layers = Flatten()(Layers)\n",
    "Layers = Dense(500)(Layers)\n",
    "Layers = Activation(\"relu\")(Layers)\n",
    "Layers = Dense(nb_classes)(Layers)\n",
    "PredictionLayer = Activation(\"softmax\", name =\"error_loss\")(Layers)\n",
    "\n",
    "# Fianlly, we create a model object:\n",
    "model = Model(inputs=[InputLayer], outputs=[PredictionLayer])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 12, 12, 25)        650       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 5, 5, 50)          11300     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1250)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               625500    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      " error_loss (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642,460\n",
      "Trainable params: 642,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---------------------------------------------------------------------------------------------------\n",
    "Next, we train the network for 100 epochs with the Adam optimizer. Let's see where our model gets us..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:48:45.670552Z",
     "start_time": "2025-10-16T09:48:45.653034Z"
    }
   },
   "source": [
    "from keras import optimizers\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer= opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\",},\n",
    "               metrics=[\"accuracy\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:49:20.414867Z",
     "start_time": "2025-10-16T09:48:45.673922Z"
    }
   },
   "source": [
    "model.fit({\"input\": X_train, }, {\"error_loss\": Y_train},\n",
    "          epochs=epochs, batch_size = batch_size,\n",
    "          verbose = 0, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:48:45.719055: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2025-10-16 11:48:45.720840: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2025-10-16 11:48:45.729452: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv2d_1/kernel/m/Assign' id:286 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv2d_1/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv2d_1/kernel/m, training/Adam/conv2d_1/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2025-10-16 11:48:47.240588: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/mul' id:176 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/error_loss_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9892\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***Note: *** The model should end up with approx. 0.9% error rate.\n",
    "___________________________________\n",
    "Fianlly, we save the model in case we need to reload it later, e.g. if you want to play around with the code ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:49:20.486100Z",
     "start_time": "2025-10-16T09:49:20.456849Z"
    }
   },
   "source": [
    "keras.models.save_model(model, \"./my_pretrained_net.h5\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "________________________________________\n",
    "__________________________________________\n",
    "\n",
    "## PART 2: Re-training the network with an empirical prior\n",
    "\n",
    "_____________________________________________________\n",
    "\n",
    "First of all, we load our pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:49:20.645503Z",
     "start_time": "2025-10-16T09:49:20.488569Z"
    }
   },
   "source": [
    "pretrained_model = keras.models.load_model(\"./my_pretrained_net.h5\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 11:49:20.534078: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_1_1/bias/Assign' id:489 op device:{requested: '', assigned: ''} def:{{{node conv2d_1_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_1_1/bias, conv2d_1_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-10-16 11:49:20.599582: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1_1/bias/v/Assign' id:770 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1_1/bias/v, dense_1_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Following, we will initialize a 16 component Gaussian mixture as our empirical prior. We will learn all parameters in the prior but the mean and the mixing proportion of the zero component, we set $\\mu_0=0$ and $\\pi_0=0.99$, respectively. Furthermore, we put a Gamma hyper-prior on the precisions of the Gaussian mixture. We set the mean such that the expected variance is $0.02$. The variance of the hyper-prior is an estimate of how strongly the variance is regularized. Note that, the variance of the zero component has much more data (i.e. weight) evidence than the other components thus we put a stronger prior on it. Somewhat counterintuitive we found it beneficial to have wider and thus noisier expected variances.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:49:21.277614Z",
     "start_time": "2025-10-16T09:49:20.652452Z"
    }
   },
   "source": [
    "from empirical_priors import GaussianMixturePrior\n",
    "from extended_keras import extract_weights\n",
    "\n",
    "pi_zero = 0.99\n",
    "\n",
    "RegularizationLayer = GaussianMixturePrior(nb_components=16, \n",
    "                                           network_weights=extract_weights(model),\n",
    "                                           pretrained_weights=pretrained_model.get_weights(), \n",
    "                                           pi_zero=pi_zero,\n",
    "                                           name=\"complexity_loss\")(Layers)\n",
    "\n",
    "model = Model(inputs = [InputLayer], outputs = [PredictionLayer, RegularizationLayer])"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We optimize the network again with ADAM, the learning rates for the network parameters, means, variances and mixing proportions may differ though."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:49:21.324674Z",
     "start_time": "2025-10-16T09:49:21.281117Z"
    }
   },
   "source": [
    "import optimizers \n",
    "from extended_keras import identity_objective\n",
    "\n",
    "tau = 0.003\n",
    "N = X_train.shape[0] \n",
    "\n",
    "opt = optimizers.Adam(lr = [5e-4,1e-4,3e-3,3e-3],  #[unnamed, means, log(precition), log(mixing proportions)]\n",
    "                      param_types_dict = ['means','gammas','rhos'])\n",
    "\n",
    "model.compile(optimizer = opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\", \"complexity_loss\": identity_objective},\n",
    "              loss_weights = {\"error_loss\": 1. , \"complexity_loss\": tau/N},\n",
    "              metrics = {'error_loss': ['accuracy'], 'complexity_loss': []})"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We train our network for 30 epochs, each taking about 45s. You can watch the progress yourself. At each epoch, we compare the original weight distribution (histogram top) to the current distribution (log-scaled histogram right). The joint scatter plot in the middle shows how each weight changed.\n",
    "\n",
    "*Note* that we had to scale the histogram logarithmically otherwise it would be little informative due to the zero spike."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from extended_keras import VisualisationCallback\n",
    "\n",
    "epochs = 30\n",
    "model.fit({\"input\": X_train,},\n",
    "          {\"error_loss\" : Y_train, \"complexity_loss\": np.zeros((N,1))},\n",
    "          epochs=epochs,\n",
    "          batch_size = batch_size,\n",
    "          verbose = 1., callbacks=[VisualisationCallback(model,X_test,Y_test, epochs)])\n",
    "\n",
    "display.clear_output()"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "24832/60000 [===========>..................] - ETA: 4s - loss: -0.0685 - error_loss_loss: 0.0292 - complexity_loss_loss: -1365213.2500 - error_loss_accuracy: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mextended_keras\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m VisualisationCallback\n\u001B[1;32m      3\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m          \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43merror_loss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcomplexity_loss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m          \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mVisualisationCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m display\u001B[38;5;241m.\u001B[39mclear_output()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/engine/training_v1.py:856\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_call_args(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    855\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_training_loop(x)\n\u001B[0;32m--> 856\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/engine/training_arrays_v1.py:734\u001B[0m, in \u001B[0;36mArrayLikeTrainingLoop.fit\u001B[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[1;32m    728\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    729\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`validation_steps` should not be specified if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    730\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`validation_data` is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    731\u001B[0m         )\n\u001B[1;32m    732\u001B[0m     val_x, val_y, val_sample_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 734\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    735\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtargets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    738\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_targets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_sample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msteps_per_epoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/engine/training_arrays_v1.py:421\u001B[0m, in \u001B[0;36mmodel_iteration\u001B[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001B[0m\n\u001B[1;32m    416\u001B[0m callbacks\u001B[38;5;241m.\u001B[39m_call_batch_hook(\n\u001B[1;32m    417\u001B[0m     mode, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_index, batch_logs\n\u001B[1;32m    418\u001B[0m )\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# Get outputs.\u001B[39;00m\n\u001B[0;32m--> 421\u001B[0m batch_outs \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mins_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch_outs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    423\u001B[0m     batch_outs \u001B[38;5;241m=\u001B[39m [batch_outs]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/keras/backend.py:4608\u001B[0m, in \u001B[0;36mGraphExecutionFunction.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   4598\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   4599\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callable_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   4600\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m feed_arrays \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feed_arrays\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4604\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m session \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\n\u001B[1;32m   4605\u001B[0m ):\n\u001B[1;32m   4606\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001B[0;32m-> 4608\u001B[0m fetched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_callable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marray_vals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4609\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_fetch_callbacks(fetched[\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetches) :])\n\u001B[1;32m   4610\u001B[0m output_structure \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mpack_sequence_as(\n\u001B[1;32m   4611\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_structure,\n\u001B[1;32m   4612\u001B[0m     fetched[: \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs)],\n\u001B[1;32m   4613\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   4614\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ATDL/lib/python3.9/site-packages/tensorflow/python/client/session.py:1481\u001B[0m, in \u001B[0;36mBaseSession._Callable.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1479\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1480\u001B[0m   run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1481\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRunCallable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1482\u001B[0m \u001B[43m                                         \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1484\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[1;32m   1485\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "display.Image(url='./figures/retraining.gif')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PART 3: Post-processing\n",
    "\n",
    "Now, the only thing that is left to do is setting each weight to the mean of the component that takes most responsibility for it i.e. quantising the weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from helpers import discretesize\n",
    "\n",
    "retrained_weights = np.copy(model.get_weights())\n",
    "compressed_weights = np.copy(model.get_weights())\n",
    "compressed_weights[:-3] = discretesize(compressed_weights, pi_zero = pi_zero)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the accuracy of the reference, the retrained and the post-processed network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"MODEL ACCURACY\")\n",
    "score = pretrained_model.evaluate({'input': X_test, },{\"error_loss\" : Y_test,}, verbose=0)[1]\n",
    "print(\"Reference Network: %0.4f\" %score)\n",
    "score = model.evaluate({'input': X_test, },{\"error_loss\" : Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Retrained Network: %0.4f\" %score)\n",
    "model.set_weights(compressed_weights)\n",
    "score = model.evaluate({'input': X_test, },{\"error_loss\" : Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Post-processed Network: %0.4f\" %score)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally let us see how many weights have been pruned."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from helpers import special_flatten\n",
    "weights = special_flatten(compressed_weights[:-3]).flatten()\n",
    "print(\"Non-zero weights: %0.2f %%\" % (100.*np.count_nonzero(weights)/ weights.size) )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see in this naive implementation we got rid of 19 out of 20 weights. Furthermore note that we quantize weights with only 16 cluster means (aka 4 bit indexes). \n",
    "\n",
    "For better results (up to 0.5%) one may anneal $\\tau$, learn the mixing proportion for the zero spike with a beta prior on it for example and ideally optimize with some hyperparamter optimization of choice such as spearmint (I also wrote some example code for deep learning and spearmint).\n",
    "\n",
    "We finish this tutorial with a series of histograms showing the results of our procedure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from helpers import save_histogram\n",
    "\n",
    "save_histogram(pretrained_model.get_weights(),save=\"figures/reference\")\n",
    "save_histogram(retrained_weights,save=\"figures/retrained\")\n",
    "save_histogram(compressed_weights,save=\"figures/post-processed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "|Weight distribution before retraining | Weight distribution after retraining|  Weight distribution after post-processing  \n",
    ":-------------------------:|:-------------------------:|:------------:|:-------------------------:\n",
    "histogram|![](figures_old/reference.png)|<img src=\"./figures/retrained.png\"/>| ![](figures_old/post-processed.png)\n",
    "log-scaled histogram|![](figures_old/reference_log.png)|<img src=\"./figures/retrained_log.png\"/>| ![](figures_old/post-processed_log.png)\n",
    "_______________________________\n",
    "### *Reference*\n",
    "\n",
    "The paper \"Soft weight-sharing for Neural Network compression\" has been accepted to ICLR 2017.\n",
    "\n",
    "\n",
    "    @inproceedings{ullrich2017soft,\n",
    "    title={Soft Weight-Sharing for Neural Network Compression},\n",
    "    author={Ullrich, Karen and  Meeds, Edward and Welling, Max},\n",
    "    booktitle={ICLR 2017},\n",
    "    year={2017}\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
